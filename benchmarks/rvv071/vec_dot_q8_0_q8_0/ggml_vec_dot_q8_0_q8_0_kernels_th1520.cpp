// Auto-generated kernels for ggml_vec_dot_q8_0_q8_0 on th1520
// RVV Version: rvv_0.7.1
// Generated by VectorWeaver

#include "ggml_vec_dot_q8_0_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
// Generated function: ggml_vec_dot_q8_0_q8_0_rvv_intrinsics
// Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_rvv_intrinsics(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    float sumf = 0.0f;
    for (int ib = 0; ib < nb; ++ib) {
        size_t vl = QK8_0;
        vint8m2_t bx_0 = __riscv_vle8_v_i8m2(x[ib].qs, vl);
        vint8m2_t by_0 = __riscv_vle8_v_i8m2(y[ib].qs, vl);
        vint16m4_t vw_mul = __riscv_vwmul_vv_i16m4(bx_0, by_0, vl);
        vint32m1_t v_zero = __riscv_vmv_v_x_i32m1(0, vl);
        vint32m1_t v_sum = __riscv_vwredsum_vs_i16m4_i32m1(vw_mul, v_zero, vl);
        int32_t sumi = __riscv_vmv_x_s_i32m1_i32(v_sum);
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    *s = sumf;
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[x1_ptr])\n\t"
            "th.vle.v v12, (%[y0_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "li t0, 64\n\t"
            "th.vsetvli x0, t0, e8, m4\n\t"
            "th.vwmul.vv v16, v8, v12\n\t"
            "li t0, 32\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v28, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vwredsum.vs v28, v20, v28\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v28\n\t"

            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[x1_ptr])\n\t"
            "th.vle.v v12, (%[y0_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "li t0, 64\n\t"
            "th.vsetvli x0, t0, e8, m4\n\t"
            "th.vwmul.vv v16, v8, v12\n\t"
            "li t0, 32\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v28, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vwredsum.vs v28, v20, v28\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v28\n\t"

            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_baseline(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_baseline
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_baseline(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v12, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vwredsum.vs v16, v12, v16\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_baseline(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v12, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vwredsum.vs v16, v12, v16\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "th.vwmul.vv v16, v12, v14\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v16, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "th.vwmul.vv v16, v12, v14\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v16, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v26, (%[x_pf0_ptr])\n\t"
            "th.vle.v v28, (%[y_pf0_ptr])\n\t"

            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "th.vwmul.vv v16, v12, v14\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v16, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v26, (%[x_pf0_ptr])\n\t"
            "th.vle.v v28, (%[y_pf0_ptr])\n\t"

            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            "th.vwmul.vv v16, v12, v14\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v16, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // 1. Load all data for current compute window
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vwmul.vv v20, v12, v14\n\t"
            // 3. Perform all reductions
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v20, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // 1. Load all data for current compute window
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vwmul.vv v20, v12, v14\n\t"
            // 3. Perform all reductions
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v20, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v26, (%[x_pf0_ptr])\n\t"
            "th.vle.v v28, (%[y_pf0_ptr])\n\t"

            // 1. Load all data for current compute window
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vwmul.vv v20, v12, v14\n\t"
            // 3. Perform all reductions
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v20, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v26, (%[x_pf0_ptr])\n\t"
            "th.vle.v v28, (%[y_pf0_ptr])\n\t"

            // 1. Load all data for current compute window
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vle.v v12, (%[x1_ptr])\n\t"
            "th.vle.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "th.vwmul.vv v16, v8, v10\n\t"
            "th.vwmul.vv v20, v12, v14\n\t"
            // 3. Perform all reductions
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vwredsum.vs v24, v16, v24\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vwredsum.vs v25, v20, v25\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v24\n\t"
            "th.vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_batch4(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
void ggml_vec_dot_q8_0_q8_0_asm_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v12, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vwredsum.vs v16, v12, v16\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_batch4(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)

// Helper function for tail loop
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "th.vsetvli x0, t0, e8,m2\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vle.v v10, (%[y_ptr])\n\t"
        "th.vwmul.vv v12, v8, v10\n\t"
        "th.vsetvli x0, t0, e16,m4\n\t"
        "th.vmv.s.x v24, x0\n\t"
        "th.vwredsum.vs v24, v12, v24\n\t"
        "th.vsetvli x0, t0, e32,m1\n\t"
        "th.vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
void ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "th.vsetvli x0, t0, e8, m2\n\t"
            "th.vle.v v8, (%[x0_ptr])\n\t"
            "th.vle.v v10, (%[y0_ptr])\n\t"
            "th.vwmul.vv v12, v8, v10\n\t"
            "th.vsetvli x0, t0, e16, m4\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vwredsum.vs v16, v12, v16\n\t"

            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q8_0_q8_0_asm_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vmv.s.x v26, x0\n\t"
            "th.vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0_lo])\n\t"
            "th.vle.v v1, (%[y0_lo])\n\t"
            "th.vle.v v2, (%[x0_hi])\n\t"
            "th.vle.v v3, (%[y0_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x1_lo])\n\t"
            "th.vle.v v1, (%[y1_lo])\n\t"
            "th.vle.v v2, (%[x1_hi])\n\t"
            "th.vle.v v3, (%[y1_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x2_lo])\n\t"
            "th.vle.v v1, (%[y2_lo])\n\t"
            "th.vle.v v2, (%[x2_hi])\n\t"
            "th.vle.v v3, (%[y2_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x3_lo])\n\t"
            "th.vle.v v1, (%[y3_lo])\n\t"
            "th.vle.v v2, (%[x3_hi])\n\t"
            "th.vle.v v3, (%[y3_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[s0], v24\n\t"
            "th.vmv.x.s %[s1], v25\n\t"
            "th.vmv.x.s %[s2], v26\n\t"
            "th.vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_lo] "r"(x[ib+0].qs), [x0_hi] "r"(x[ib+0].qs + 16), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_lo] "r"(x[ib+1].qs), [x1_hi] "r"(x[ib+1].qs + 16), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_lo] "r"(x[ib+2].qs), [x2_hi] "r"(x[ib+2].qs + 16), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_lo] "r"(x[ib+3].qs), [x3_hi] "r"(x[ib+3].qs + 16), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16", "v24", "v25", "v26", "v27"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vmv.s.x v26, x0\n\t"
            "th.vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0_lo])\n\t"
            "th.vle.v v1, (%[y0_lo])\n\t"
            "th.vle.v v2, (%[x0_hi])\n\t"
            "th.vle.v v3, (%[y0_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x1_lo])\n\t"
            "th.vle.v v1, (%[y1_lo])\n\t"
            "th.vle.v v2, (%[x1_hi])\n\t"
            "th.vle.v v3, (%[y1_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x2_lo])\n\t"
            "th.vle.v v1, (%[y2_lo])\n\t"
            "th.vle.v v2, (%[x2_hi])\n\t"
            "th.vle.v v3, (%[y2_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x3_lo])\n\t"
            "th.vle.v v1, (%[y3_lo])\n\t"
            "th.vle.v v2, (%[x3_hi])\n\t"
            "th.vle.v v3, (%[y3_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[s0], v24\n\t"
            "th.vmv.x.s %[s1], v25\n\t"
            "th.vmv.x.s %[s2], v26\n\t"
            "th.vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_lo] "r"(x[ib+0].qs), [x0_hi] "r"(x[ib+0].qs + 16), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_lo] "r"(x[ib+1].qs), [x1_hi] "r"(x[ib+1].qs + 16), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_lo] "r"(x[ib+2].qs), [x2_hi] "r"(x[ib+2].qs + 16), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_lo] "r"(x[ib+3].qs), [x3_hi] "r"(x[ib+3].qs + 16), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16", "v24", "v25", "v26", "v27"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_lo])\n\t"
            "th.vle.v v1, (%[y_lo])\n\t"
            "th.vle.v v2, (%[x_hi])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmacc.vv v8, v2, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [x_hi] "r"(x[ib].qs + 16), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0])\n\t"
            "th.vle.v v1, (%[y0])\n\t"
            "th.vle.v v2, (%[x1])\n\t"
            "th.vle.v v3, (%[y1])\n\t"
            "th.vwmul.vv v8, v0, v1\n\t"
            "th.vwmul.vv v10, v2, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v2, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v8, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v8, v2, v8\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v8\n\t"
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif


