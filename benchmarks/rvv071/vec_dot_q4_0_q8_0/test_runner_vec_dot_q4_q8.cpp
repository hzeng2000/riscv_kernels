

// Auto-generated test runner for ggml_vec_dot_q4_0_q8_0 on th1520
// RVV Version: rvv_0.7.1
// Generated by VectorWeaver

#include <iostream>
#include <vector>
#include <string>
#include <random>
#include <chrono>
#include <cmath>
#include <iomanip>
#include <cassert>

#include "../common/common_defs.h"
#include "ggml_vec_dot_q4_0_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif

// === Test Configuration ===
struct TestConfig {
    int n = 2048;
    int runs = 10000;
};

// === Test Entry ===
struct TestEntry {
    std::string name;
    vec_dot_q4_0_q8_0_t func;
};

// === Helper Functions ===
std::string short_name(const std::string& full_name) {
    const std::string prefix = "ggml_vec_dot_q4_0_q8_0_";
    if (full_name.find(prefix) == 0) {
        return full_name.substr(prefix.length());
    }
    return full_name;
}

void generate_random_floats(std::vector<float>& vec) {
    std::mt19937 gen(1337);
    std::uniform_real_distribution<> dis(-1.0f, 1.0f);
    for (float& val : vec) { val = dis(gen); }
}

void quantize_to_q4_0(const std::vector<float>& in, std::vector<block_q4_0>& out) {
    const int nb = in.size() / QK4_0;
    out.resize(nb);
    for (int i = 0; i < nb; ++i) {
        float amax = 0.0f;
        for (int j = 0; j < QK4_0; ++j) {
            amax = std::max(amax, std::abs(in[i*QK4_0 + j]));
        }
        const float d = amax / 7.0f;
        out[i].d = GGML_FP32_TO_FP16(d);
        const float id = d ? 1.0f/d : 0.0f;
        for (int j = 0; j < QK4_0/2; ++j) {
            const int v0 = (int)roundf(in[i*QK4_0 + j] * id) + 8;
            const int v1 = (int)roundf(in[i*QK4_0 + j + QK4_0/2] * id) + 8;
            out[i].qs[j] = (v0 & 0x0F) | ((v1 & 0x0F) << 4);
        }
    }
}

void quantize_to_q8_0(const std::vector<float>& in, std::vector<block_q8_0>& out) {
    const int nb = in.size() / QK8_0;
    out.resize(nb);
    for (int i = 0; i < nb; ++i) {
        float amax = 0.0f;
        for (int j = 0; j < QK8_0; ++j) {
            amax = std::max(amax, std::abs(in[i*QK8_0 + j]));
        }
        const float d = amax / 127.0f;
        out[i].d = GGML_FP32_TO_FP16(d);
        const float id = d ? 1.0f/d : 0.0f;
        for (int j = 0; j < QK8_0; ++j) {
            out[i].qs[j] = (int8_t)roundf(in[i*QK8_0 + j] * id);
        }
    }
}

// === Scalar Baseline ===
void ggml_vec_dot_q4_0_q8_0_scalar(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *x = (const block_q4_0 *)vx;
    const block_q8_0 *y = (const block_q8_0 *)vy;
    float sumf = 0.0f;
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi = 0;
        for (int j = 0; j < QK8_0/2; ++j) {
            const int v0 = (x[ib].qs[j] & 0x0F) - 8;
            const int v1 = (x[ib].qs[j] >> 4) - 8;
            sumi += v0 * y[ib].qs[j] + v1 * y[ib].qs[j + QK8_0/2];
        }
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    *s = sumf;
}

void print_help(char* app_name) {
    std::cout << "Usage: " << app_name << " [options]\n";
    std::cout << "  -n <size>    Data size (default: 4096)\n";
    std::cout << "  -r <runs>    Number of runs (default: 1000)\n";
    std::cout << "  --help       Show this help\n";
}

void parse_args(int argc, char** argv, TestConfig& config) {
    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg == "-n") { config.n = std::stoi(argv[++i]); }
        else if (arg == "-r") { config.runs = std::stoi(argv[++i]); }
        else if (arg == "--help") { print_help(argv[0]); exit(0); }
    }
}

int main(int argc, char** argv) {
    TestConfig config;
    parse_args(argc, argv, config);
    
    std::cout << "=== ggml_vec_dot_q4_0_q8_0 Benchmark on th1520 ===" << std::endl;
    std::cout << "RVV Version: rvv_0.7.1" << std::endl;
    std::cout << "Data Size: " << config.n << ", Runs: " << config.runs << std::endl;
    std::cout << std::endl;
    
    // Setup test data
    std::vector<float> x_f32(config.n), y_f32(config.n);
    generate_random_floats(x_f32);
    generate_random_floats(y_f32);
    std::vector<block_q4_0> x_q4_0;
    std::vector<block_q8_0> y_q8_0;
    quantize_to_q4_0(x_f32, x_q4_0);
    quantize_to_q8_0(y_f32, y_q8_0);
    
    // Register all kernels
    std::vector<TestEntry> all_tests;
    all_tests.push_back({"Scalar (baseline)", ggml_vec_dot_q4_0_q8_0_scalar});
    

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
    all_tests.push_back({"RVV Intrinsics", ggml_vec_dot_q4_0_q8_0_rvv_intrinsics});
#endif


#if defined(__RVV_ASM_XTHEAD)
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_baseline", ggml_vec_dot_q4_0_q8_0_baseline});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64", ggml_vec_dot_q4_0_q8_0_asm_f64});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2", ggml_vec_dot_q4_0_q8_0_asm_unroll2});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64", ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_batch4", ggml_vec_dot_q4_0_q8_0_asm_batch4});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64_batch4", ggml_vec_dot_q4_0_q8_0_asm_f64_batch4});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_split_macc", ggml_vec_dot_q4_0_q8_0_asm_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc", ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc", ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4", ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4", ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_split", ggml_vec_dot_q4_0_q8_0_asm_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64_split", ggml_vec_dot_q4_0_q8_0_asm_f64_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_split", ggml_vec_dot_q4_0_q8_0_asm_unroll2_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split", ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split", ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_split", ggml_vec_dot_q4_0_q8_0_asm_unroll4_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split", ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split", ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split", ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_split_batch4", ggml_vec_dot_q4_0_q8_0_asm_split_batch4});
    all_tests.push_back({"ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4", ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4});
#endif

    // === Step 1: Correctness Check ===
    std::cout << "=== Correctness Verification ===" << std::endl;
    
    float baseline_result = 0.0f;
    ggml_vec_dot_q4_0_q8_0_scalar(config.n, &baseline_result, x_q4_0.data(), y_q8_0.data());
    
    const float tolerance = 1e-3f;
    bool all_correct = true;
    
    for (auto const& test : all_tests) {
        float result = 0.0f;
        test.func(config.n, &result, x_q4_0.data(), y_q8_0.data());
        float rel_err = std::abs(result - baseline_result) / (std::abs(baseline_result) + 1e-9f);
        bool correct = rel_err < tolerance;
        if (!correct) all_correct = false;
        std::cout << "  " << test.name << ": " << (correct ? "PASS" : "FAIL") 
                  << " (rel_err=" << rel_err << ")" << std::endl;
    }
    
    if (!all_correct) {
        std::cerr << "\n[WARNING] Some kernels failed correctness check!\n" << std::endl;
    }
    
    // === Step 2: Performance Test ===
    std::cout << "\n=== Performance Benchmark ===" << std::endl;
    std::cout << std::left << std::setw(50) << "Kernel Variant"
              << std::setw(18) << "Avg Time (us)"
              << "Speedup" << std::endl;
    std::cout << std::string(80, '-') << std::endl;
    
    double baseline_time = 0.0;
    for (size_t i = 0; i < all_tests.size(); ++i) {
        auto const& test = all_tests[i];
        float result = 0.0f;
        
        // Warmup
        for (int w = 0; w < 10; ++w) {
            test.func(config.n, &result, x_q4_0.data(), y_q8_0.data());
        }
        
        auto start = std::chrono::high_resolution_clock::now();
        for (int r = 0; r < config.runs; ++r) {
            test.func(config.n, &result, x_q4_0.data(), y_q8_0.data());
        }
        auto end = std::chrono::high_resolution_clock::now();
        
        double avg_time = std::chrono::duration<double, std::micro>(end - start).count() / config.runs;
        
        if (i == 0) baseline_time = avg_time;
        double speedup = baseline_time / avg_time;
        
        std::cout << std::left << std::setw(50) << short_name(test.name)
                  << std::fixed << std::setprecision(4) << std::setw(18) << avg_time
                  << std::setprecision(4) << speedup << "x" << std::endl;
    }
    
    return 0;
}