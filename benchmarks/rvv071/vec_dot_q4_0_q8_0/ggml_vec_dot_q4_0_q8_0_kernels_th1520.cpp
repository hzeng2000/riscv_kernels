// Auto-generated kernels for ggml_vec_dot_q4_0_q8_0 on th1520
// RVV Version: rvv_0.7.1
// Generated by VectorWeaver

#include "ggml_vec_dot_q4_0_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__riscv_v) || defined(__riscv_xtheadvector)
void ggml_vec_dot_q4_0_q8_0_rvv_intrinsics(int n, float *s, const void *vx, const void *vy) {
    const int qk = QK8_0;
    const int nb = n / qk;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
    assert(n % qk == 0);

    float sumf = 0.0f;
    for (int ib = 0; ib < nb; ++ib) {
        size_t vl = qk / 2;
        vuint8m1_t x_packed = __riscv_vle8_v_u8m1(x[ib].qs, vl);
        vuint8m1_t x_lower_u = __riscv_vand_vx_u8m1(x_packed, 0x0F, vl);
        vuint8m1_t x_upper_u = __riscv_vsrl_vx_u8m1(x_packed, 4, vl);
        vint8m1_t x_lower = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(x_lower_u), 8, vl);
        vint8m1_t x_upper = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(x_upper_u), 8, vl);
        vint8m1_t y_0 = __riscv_vle8_v_i8m1(&y[ib].qs[0], vl);
        vint8m1_t y_1 = __riscv_vle8_v_i8m1(&y[ib].qs[qk/2], vl);
        vint16m2_t sum_w = __riscv_vwmul_vv_i16m2(x_lower, y_0, vl);
        sum_w = __riscv_vwmacc_vv_i16m2(sum_w, x_upper, y_1, vl);
        vint32m1_t v_zero = __riscv_vmv_v_x_i32m1(0, vl);
        vint32m1_t v_sum = __riscv_vwredsum_vs_i16m2_i32m1(sum_w, v_zero, vl);
        int32_t sumi = __riscv_vmv_x_s_i32m1_i32(v_sum);
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    *s = sumf;
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_baseline(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_baseline(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
float sumf = 0.0f;    int ib = 0;

    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"

            :  [sumi0] "=r"(sumi[0])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v8", "v9"        );
sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));    }
    
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_baseline(&x[ib], &y[ib], &sumf); }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
double sum_d = 0.0;    int ib = 0;

    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"

            :  [sumi0] "=r"(sumi[0])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v8", "v9"        );
sum_d += (double)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));    }
    
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64(&x[ib], &y[ib], &temp_sumf); }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_unroll2(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
float sumf = 0.0f;    int ib = 0;

    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"
                // --- Block 1 ---
                "th.vle.v v16, (%[x1_ptr])\n\t"
                "th.vle.v v18, (%[y1_lo_ptr])\n\t"
                "th.vle.v v19, (%[y1_hi_ptr])\n\t"
                "th.vand.vi v17, v16, 15\n\t"
                "th.vsrl.vi v16, v16, 4\n\t"
                "th.vadd.vi v17, v17, -8\n\t"
                "th.vadd.vi v16, v16, -8\n\t"
                "th.vwmul.vv v20, v17, v18\n\t"
                "th.vwmacc.vv v20, v16, v19\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vmv.s.x v22, x0\n\t"
            "th.vwredsum.vs v22, v20, v22\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"
            "th.vmv.x.s %[sumi1], v22\n\t"

            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs), 
              [y1_hi_ptr] "r"(y[ib+1].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9"        );
sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));    }
    
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2(&x[ib], &y[ib], &sumf); }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
double sum_d = 0.0;    int ib = 0;

    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"
                // --- Block 1 ---
                "th.vle.v v16, (%[x1_ptr])\n\t"
                "th.vle.v v18, (%[y1_lo_ptr])\n\t"
                "th.vle.v v19, (%[y1_hi_ptr])\n\t"
                "th.vand.vi v17, v16, 15\n\t"
                "th.vsrl.vi v16, v16, 4\n\t"
                "th.vadd.vi v17, v17, -8\n\t"
                "th.vadd.vi v16, v16, -8\n\t"
                "th.vwmul.vv v20, v17, v18\n\t"
                "th.vwmacc.vv v20, v16, v19\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vmv.s.x v22, x0\n\t"
            "th.vwredsum.vs v22, v20, v22\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"
            "th.vmv.x.s %[sumi1], v22\n\t"

            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs), 
              [y1_hi_ptr] "r"(y[ib+1].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9"        );
sum_d += (double)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));sum_d += (double)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));    }
    
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(&x[ib], &y[ib], &temp_sumf); }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
float sumf = 0.0f;    int ib = 0;

    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vle.v v16, (%[x1_ptr])\n\t"
                "th.vle.v v18, (%[y1_lo_ptr])\n\t"
                "th.vle.v v19, (%[y1_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vand.vi v17, v16, 15\n\t"
                "th.vsrl.vi v16, v16, 4\n\t"
                "th.vadd.vi v17, v17, -8\n\t"
                "th.vadd.vi v16, v16, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"
                "th.vwmul.vv v20, v17, v18\n\t"
                "th.vwmacc.vv v20, v16, v19\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vmv.s.x v22, x0\n\t"
            "th.vwredsum.vs v22, v20, v22\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"
            "th.vmv.x.s %[sumi1], v22\n\t"

            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs), 
              [y1_hi_ptr] "r"(y[ib+1].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9"        );
sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));    }
    
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(&x[ib], &y[ib], &sumf); }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
double sum_d = 0.0;    int ib = 0;

    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vle.v v16, (%[x1_ptr])\n\t"
                "th.vle.v v18, (%[y1_lo_ptr])\n\t"
                "th.vle.v v19, (%[y1_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vand.vi v17, v16, 15\n\t"
                "th.vsrl.vi v16, v16, 4\n\t"
                "th.vadd.vi v17, v17, -8\n\t"
                "th.vadd.vi v16, v16, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"
                "th.vwmul.vv v20, v17, v18\n\t"
                "th.vwmacc.vv v20, v16, v19\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vmv.s.x v22, x0\n\t"
            "th.vwredsum.vs v22, v20, v22\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"
            "th.vmv.x.s %[sumi1], v22\n\t"

            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs), 
              [y1_hi_ptr] "r"(y[ib+1].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9"        );
sum_d += (double)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));sum_d += (double)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));    }
    
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(&x[ib], &y[ib], &temp_sumf); }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_batch4(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
float sumf = 0.0f;    int ib = 0;

    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"

            :  [sumi0] "=r"(sumi[0])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v8", "v9"        );
sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));    }
    
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_batch4(&x[ib], &y[ib], &sumf); }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_XTHEAD)

static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "th.vsetvli x0, t0, e8, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vand.vi v9, v8, 15\n\t"
        "th.vsrl.vi v8, v8, 4\n\t"
        "th.vadd.vi v9, v9, -8\n\t"
        "th.vadd.vi v8, v8, -8\n\t"
        "th.vle.v v10, (%[y_lo_ptr])\n\t"
        "th.vle.v v11, (%[y_hi_ptr])\n\t"
        "th.vwmul.vv v12, v9, v10\n\t"
        "th.vwmacc.vv v12, v8, v11\n\t"
        "th.vsetvli x0, t0, e16, m2\n\t"
        "th.vmv.s.x v16, x0\n\t"
        "th.vwredsum.vs v16, v12, v16\n\t"
        "th.vsetvli x0, t0, e32, m1\n\t"
        "th.vmv.x.s %[sumi], v16\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_lo_ptr] "r"(y->qs), [y_hi_ptr] "r"(y->qs + 16)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

void ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 * GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 * GGML_RESTRICT y = (const block_q8_0 *)vy;
double sum_d = 0.0;    int ib = 0;

    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            
                // --- Block 0 ---
                "th.vle.v v8, (%[x0_ptr])\n\t"
                "th.vle.v v10, (%[y0_lo_ptr])\n\t"
                "th.vle.v v11, (%[y0_hi_ptr])\n\t"
                "th.vand.vi v9, v8, 15\n\t"
                "th.vsrl.vi v8, v8, 4\n\t"
                "th.vadd.vi v9, v9, -8\n\t"
                "th.vadd.vi v8, v8, -8\n\t"
                "th.vwmul.vv v12, v9, v10\n\t"
                "th.vwmacc.vv v12, v8, v11\n\t"

            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vmv.s.x v14, x0\n\t"
            "th.vwredsum.vs v14, v12, v14\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi0], v14\n\t"

            :  [sumi0] "=r"(sumi[0])             :               [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs), 
              [y0_hi_ptr] "r"(y[ib+0].qs + 16)            : "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v8", "v9"        );
sum_d += (double)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));    }
    
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) { process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(&x[ib], &y[ib], &temp_sumf); }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vmv.s.x v26, x0\n\t"
            "th.vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0_ptr])\n\t"
            "th.vle.v v3, (%[y0_lo])\n\t"
            "th.vle.v v4, (%[y0_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x1_ptr])\n\t"
            "th.vle.v v3, (%[y1_lo])\n\t"
            "th.vle.v v4, (%[y1_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x2_ptr])\n\t"
            "th.vle.v v3, (%[y2_lo])\n\t"
            "th.vle.v v4, (%[y2_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x3_ptr])\n\t"
            "th.vle.v v3, (%[y3_lo])\n\t"
            "th.vle.v v4, (%[y3_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v27, v8, v27\n\t"
            
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[s0], v24\n\t"
            "th.vmv.x.s %[s1], v25\n\t"
            "th.vmv.x.s %[s2], v26\n\t"
            "th.vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_ptr] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_ptr] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_ptr] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_ptr] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v24", "v25", "v26", "v27", "v3", "v4", "v8", "v9"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v24, x0\n\t"
            "th.vmv.s.x v25, x0\n\t"
            "th.vmv.s.x v26, x0\n\t"
            "th.vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x0_ptr])\n\t"
            "th.vle.v v3, (%[y0_lo])\n\t"
            "th.vle.v v4, (%[y0_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x1_ptr])\n\t"
            "th.vle.v v3, (%[y1_lo])\n\t"
            "th.vle.v v4, (%[y1_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x2_ptr])\n\t"
            "th.vle.v v3, (%[y2_lo])\n\t"
            "th.vle.v v4, (%[y2_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x3_ptr])\n\t"
            "th.vle.v v3, (%[y3_lo])\n\t"
            "th.vle.v v4, (%[y3_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v2, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v2, v2, -8\n\t"
            "th.vwmul.vv v8, v1, v3\n\t"
            "th.vwmacc.vv v8, v2, v4\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v27, v8, v27\n\t"
            
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[s0], v24\n\t"
            "th.vmv.x.s %[s1], v25\n\t"
            "th.vmv.x.s %[s2], v26\n\t"
            "th.vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_ptr] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_ptr] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_ptr] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_ptr] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v24", "v25", "v26", "v27", "v3", "v4", "v8", "v9"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmacc.vv v8, v0, v3\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */




#if defined(__RVV_ASM_XTHEAD)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: XTheadVector (RVV 0.7.1), Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "th.vsetvli x0, t0, e8, m1\n\t"
            "th.vle.v v0, (%[x_ptr])\n\t"
            "th.vle.v v2, (%[y_lo])\n\t"
            "th.vle.v v3, (%[y_hi])\n\t"
            "th.vand.vi v1, v0, 15\n\t"
            "th.vsrl.vi v0, v0, 4\n\t"
            "th.vadd.vi v1, v1, -8\n\t"
            "th.vadd.vi v0, v0, -8\n\t"
            "th.vwmul.vv v8, v1, v2\n\t"
            "th.vwmul.vv v10, v0, v3\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vadd.vv v8, v8, v10\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.s.x v16, x0\n\t"
            "th.vsetvli x0, t0, e16, m2\n\t"
            "th.vwredsum.vs v16, v8, v16\n\t"
            "th.vsetvli x0, t0, e32, m1\n\t"
            "th.vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif


