// Auto-generated kernels for ggml_vec_silu_f32 on th1520
// RVV Version: rvv_0.7.1
// Generated by VectorWeaver

#include "ggml_vec_silu_f32_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__riscv_v) || defined(__riscv_xtheadvector)
void ggml_vec_silu_f32_rvv_intrinsics(const int n, float * y, const float * x) {
    // ... (Paste your full, correct intrinsics implementation here) ...
    int i = 0;
    for (; i < n; ) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        vfloat32m1_t vx = __riscv_vle32_v_f32m1(x + i, vl);
        vfloat32m1_t vneg_x = __riscv_vfneg_v_f32m1(vx, vl);
        vfloat32m1_t vz_clamped = __riscv_vfmax_vf_f32m1(vneg_x, -87.3f, vl);
        vz_clamped = __riscv_vfmin_vf_f32m1(vz_clamped, 88.7f, vl);
        const float exp_alpha_f = 12102203.0f;
        const int32_t exp_bias_i  = 1065353216;
        vfloat32m1_t vscaled_z = __riscv_vfmul_vf_f32m1(vz_clamped, exp_alpha_f, vl);
        vint32m1_t vint_z = __riscv_vfcvt_x_f_v_i32m1(vscaled_z, vl);
        vint32m1_t vexp_int = __riscv_vadd_vx_i32m1(vint_z, exp_bias_i, vl);
        vfloat32m1_t vexp_val = __riscv_vreinterpret_v_i32m1_f32m1(vexp_int);
        const float one_f = 1.0f;
        vfloat32m1_t vden = __riscv_vfadd_vf_f32m1(vexp_val, one_f, vl);
        vfloat32m1_t vy = __riscv_vfdiv_vv_f32m1(vx, vden, vl);
        __riscv_vse32_v_f32m1(y + i, vy, vl);
        i += vl;
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)
// Helper for tail processing
static inline void process_silu_tail_asm_fast_ggml_vec_silu_f32_baseline(float* y, const float* x, size_t vl) {
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    asm volatile (
        "th.vsetvli x0, %[vl], e32, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vfneg.v v9, v8\n\t"
        "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
        "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
        "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
        "th.vfcvt.x.f.v v10, v9\n\t"
        "th.vadd.vx v10, v10, %[exp_bias]\n\t"
        "th.vfadd.vf v9, v10, %[one]\n\t" // v9 is now the denominator
        "th.vfdiv.vv v8, v8, v9\n\t"
        "th.vse.v v8, (%[y_ptr])\n\t"
        : : [x_ptr] "r"(x), [y_ptr] "r"(y), [vl] "r"(vl),
          [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
          [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
        : "t0", "memory", "v8", "v9", "v10"
    );
}

void ggml_vec_silu_f32_baseline(const int n, float * y, const float * x) {
    int i = 0;
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    
    while (i < n) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        
        // Check if we can process `ur` full chunks
        bool can_unroll = true;
        for (int j=1; j < 1; ++j) {
            if (vl * j >= (size_t)(n-i)) {
                can_unroll = false;
                break;
            }
        }
        
        // If unroll_factor is 1, we can't unroll, so this block is effectively disabled.
        // We add a `false` condition to make the C++ compiler optimize it away.
        if (false) {
            size_t vl_bytes = vl * 4;
            asm volatile (
                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v8, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v9, v8\n\t"
                    "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
                    "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
                    "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v10, v9\n\t"
                    "th.vadd.vx v10, v10, %[exp_bias]\n\t"
                    "th.vfadd.vf v9, v10, %[one]\n\t"
                    "th.vfdiv.vv v8, v8, v9\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v8, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"


                : [x_ptr] "+r"(x), [y_ptr] "+r"(y)
                : [vl] "r"(vl), [vl_bytes] "r"(vl_bytes),
                  [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
                  [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
                : "memory", "t0", "v10", "v8", "v9"            );
            i += vl * 1;
        } else {
            // Process remaining tail or single chunk
            process_silu_tail_asm_fast_ggml_vec_silu_f32_baseline(y + i, x + i, vl);
            i += vl;
        }
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)
// Helper for tail processing
static inline void process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll2(float* y, const float* x, size_t vl) {
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    asm volatile (
        "th.vsetvli x0, %[vl], e32, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vfneg.v v9, v8\n\t"
        "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
        "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
        "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
        "th.vfcvt.x.f.v v10, v9\n\t"
        "th.vadd.vx v10, v10, %[exp_bias]\n\t"
        "th.vfadd.vf v9, v10, %[one]\n\t" // v9 is now the denominator
        "th.vfdiv.vv v8, v8, v9\n\t"
        "th.vse.v v8, (%[y_ptr])\n\t"
        : : [x_ptr] "r"(x), [y_ptr] "r"(y), [vl] "r"(vl),
          [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
          [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
        : "t0", "memory", "v8", "v9", "v10"
    );
}

void ggml_vec_silu_f32_asm_unroll2(const int n, float * y, const float * x) {
    int i = 0;
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    
    while (i < n) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        
        // Check if we can process `ur` full chunks
        bool can_unroll = true;
        for (int j=1; j < 2; ++j) {
            if (vl * j >= (size_t)(n-i)) {
                can_unroll = false;
                break;
            }
        }
        
        if (can_unroll) {
            size_t vl_bytes = vl * 4;
            asm volatile (
                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v8, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v9, v8\n\t"
                    "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
                    "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
                    "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v10, v9\n\t"
                    "th.vadd.vx v10, v10, %[exp_bias]\n\t"
                    "th.vfadd.vf v9, v10, %[one]\n\t"
                    "th.vfdiv.vv v8, v8, v9\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v8, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v12, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v13, v12\n\t"
                    "th.vfmax.vf v13, v13, %[clamp_min]\n\t"
                    "th.vfmin.vf v13, v13, %[clamp_max]\n\t"
                    "th.vfmul.vf v13, v13, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v14, v13\n\t"
                    "th.vadd.vx v14, v14, %[exp_bias]\n\t"
                    "th.vfadd.vf v13, v14, %[one]\n\t"
                    "th.vfdiv.vv v12, v12, v13\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v12, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"


                : [x_ptr] "+r"(x), [y_ptr] "+r"(y)
                : [vl] "r"(vl), [vl_bytes] "r"(vl_bytes),
                  [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
                  [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
                : "memory", "t0", "v10", "v12", "v13", "v14", "v8", "v9"            );
            i += vl * 2;
        } else {
            // Process remaining tail or single chunk
            process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll2(y + i, x + i, vl);
            i += vl;
        }
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)
// Helper for tail processing
static inline void process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll2_interleaved(float* y, const float* x, size_t vl) {
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    asm volatile (
        "th.vsetvli x0, %[vl], e32, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vfneg.v v9, v8\n\t"
        "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
        "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
        "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
        "th.vfcvt.x.f.v v10, v9\n\t"
        "th.vadd.vx v10, v10, %[exp_bias]\n\t"
        "th.vfadd.vf v9, v10, %[one]\n\t" // v9 is now the denominator
        "th.vfdiv.vv v8, v8, v9\n\t"
        "th.vse.v v8, (%[y_ptr])\n\t"
        : : [x_ptr] "r"(x), [y_ptr] "r"(y), [vl] "r"(vl),
          [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
          [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
        : "t0", "memory", "v8", "v9", "v10"
    );
}

void ggml_vec_silu_f32_asm_unroll2_interleaved(const int n, float * y, const float * x) {
    int i = 0;
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    
    while (i < n) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        
        // Check if we can process `ur` full chunks
        bool can_unroll = true;
        for (int j=1; j < 2; ++j) {
            if (vl * j >= (size_t)(n-i)) {
                can_unroll = false;
                break;
            }
        }
        
        if (can_unroll) {
            size_t vl_bytes = vl * 4;
            asm volatile (
                    // --- Load all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v8, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v12, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    
                    // --- Compute all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v9, v8\n\t"
                    "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
                    "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
                    "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v10, v9\n\t"
                    "th.vadd.vx v10, v10, %[exp_bias]\n\t"
                    "th.vfadd.vf v9, v10, %[one]\n\t"
                    "th.vfdiv.vv v8, v8, v9\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v13, v12\n\t"
                    "th.vfmax.vf v13, v13, %[clamp_min]\n\t"
                    "th.vfmin.vf v13, v13, %[clamp_max]\n\t"
                    "th.vfmul.vf v13, v13, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v14, v13\n\t"
                    "th.vadd.vx v14, v14, %[exp_bias]\n\t"
                    "th.vfadd.vf v13, v14, %[one]\n\t"
                    "th.vfdiv.vv v12, v12, v13\n\t"
                    
                    // --- Store all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v8, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v12, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                : [x_ptr] "+r"(x), [y_ptr] "+r"(y)
                : [vl] "r"(vl), [vl_bytes] "r"(vl_bytes),
                  [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
                  [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
                : "memory", "t0", "v10", "v12", "v13", "v14", "v8", "v9"            );
            i += vl * 2;
        } else {
            // Process remaining tail or single chunk
            process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll2_interleaved(y + i, x + i, vl);
            i += vl;
        }
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)
// Helper for tail processing
static inline void process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll4(float* y, const float* x, size_t vl) {
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    asm volatile (
        "th.vsetvli x0, %[vl], e32, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vfneg.v v9, v8\n\t"
        "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
        "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
        "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
        "th.vfcvt.x.f.v v10, v9\n\t"
        "th.vadd.vx v10, v10, %[exp_bias]\n\t"
        "th.vfadd.vf v9, v10, %[one]\n\t" // v9 is now the denominator
        "th.vfdiv.vv v8, v8, v9\n\t"
        "th.vse.v v8, (%[y_ptr])\n\t"
        : : [x_ptr] "r"(x), [y_ptr] "r"(y), [vl] "r"(vl),
          [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
          [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
        : "t0", "memory", "v8", "v9", "v10"
    );
}

void ggml_vec_silu_f32_asm_unroll4(const int n, float * y, const float * x) {
    int i = 0;
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    
    while (i < n) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        
        // Check if we can process `ur` full chunks
        bool can_unroll = true;
        for (int j=1; j < 4; ++j) {
            if (vl * j >= (size_t)(n-i)) {
                can_unroll = false;
                break;
            }
        }
        
        if (can_unroll) {
            size_t vl_bytes = vl * 4;
            asm volatile (
                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v8, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v9, v8\n\t"
                    "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
                    "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
                    "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v10, v9\n\t"
                    "th.vadd.vx v10, v10, %[exp_bias]\n\t"
                    "th.vfadd.vf v9, v10, %[one]\n\t"
                    "th.vfdiv.vv v8, v8, v9\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v8, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v12, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v13, v12\n\t"
                    "th.vfmax.vf v13, v13, %[clamp_min]\n\t"
                    "th.vfmin.vf v13, v13, %[clamp_max]\n\t"
                    "th.vfmul.vf v13, v13, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v14, v13\n\t"
                    "th.vadd.vx v14, v14, %[exp_bias]\n\t"
                    "th.vfadd.vf v13, v14, %[one]\n\t"
                    "th.vfdiv.vv v12, v12, v13\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v12, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v16, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v17, v16\n\t"
                    "th.vfmax.vf v17, v17, %[clamp_min]\n\t"
                    "th.vfmin.vf v17, v17, %[clamp_max]\n\t"
                    "th.vfmul.vf v17, v17, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v18, v17\n\t"
                    "th.vadd.vx v18, v18, %[exp_bias]\n\t"
                    "th.vfadd.vf v17, v18, %[one]\n\t"
                    "th.vfdiv.vv v16, v16, v17\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v16, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                    // --- Load one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v20, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"

                    // --- Compute one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v21, v20\n\t"
                    "th.vfmax.vf v21, v21, %[clamp_min]\n\t"
                    "th.vfmin.vf v21, v21, %[clamp_max]\n\t"
                    "th.vfmul.vf v21, v21, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v22, v21\n\t"
                    "th.vadd.vx v22, v22, %[exp_bias]\n\t"
                    "th.vfadd.vf v21, v22, %[one]\n\t"
                    "th.vfdiv.vv v20, v20, v21\n\t"

                    // --- Store one chunk ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v20, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"


                : [x_ptr] "+r"(x), [y_ptr] "+r"(y)
                : [vl] "r"(vl), [vl_bytes] "r"(vl_bytes),
                  [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
                  [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
                : "memory", "t0", "v10", "v12", "v13", "v14", "v16", "v17", "v18", "v20", "v21", "v22", "v8", "v9"            );
            i += vl * 4;
        } else {
            // Process remaining tail or single chunk
            process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll4(y + i, x + i, vl);
            i += vl;
        }
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_XTHEAD)
// Helper for tail processing
static inline void process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll4_interleaved(float* y, const float* x, size_t vl) {
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    asm volatile (
        "th.vsetvli x0, %[vl], e32, m1\n\t"
        "th.vle.v v8, (%[x_ptr])\n\t"
        "th.vfneg.v v9, v8\n\t"
        "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
        "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
        "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
        "th.vfcvt.x.f.v v10, v9\n\t"
        "th.vadd.vx v10, v10, %[exp_bias]\n\t"
        "th.vfadd.vf v9, v10, %[one]\n\t" // v9 is now the denominator
        "th.vfdiv.vv v8, v8, v9\n\t"
        "th.vse.v v8, (%[y_ptr])\n\t"
        : : [x_ptr] "r"(x), [y_ptr] "r"(y), [vl] "r"(vl),
          [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
          [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
        : "t0", "memory", "v8", "v9", "v10"
    );
}

void ggml_vec_silu_f32_asm_unroll4_interleaved(const int n, float * y, const float * x) {
    int i = 0;
    const float exp_alpha_f = 12102203.0f;
    const int32_t exp_bias_i  = 1065353216;
    const float one_f = 1.0f;
    
    while (i < n) {
        size_t vl = __riscv_vsetvl_e32m1(n - i);
        
        // Check if we can process `ur` full chunks
        bool can_unroll = true;
        for (int j=1; j < 4; ++j) {
            if (vl * j >= (size_t)(n-i)) {
                can_unroll = false;
                break;
            }
        }
        
        if (can_unroll) {
            size_t vl_bytes = vl * 4;
            asm volatile (
                    // --- Load all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v8, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v12, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v16, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vle.v v20, (%[x_ptr])\n\t"
                    "add %[x_ptr], %[x_ptr], %[vl_bytes]\n\t"
                    
                    // --- Compute all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v9, v8\n\t"
                    "th.vfmax.vf v9, v9, %[clamp_min]\n\t"
                    "th.vfmin.vf v9, v9, %[clamp_max]\n\t"
                    "th.vfmul.vf v9, v9, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v10, v9\n\t"
                    "th.vadd.vx v10, v10, %[exp_bias]\n\t"
                    "th.vfadd.vf v9, v10, %[one]\n\t"
                    "th.vfdiv.vv v8, v8, v9\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v13, v12\n\t"
                    "th.vfmax.vf v13, v13, %[clamp_min]\n\t"
                    "th.vfmin.vf v13, v13, %[clamp_max]\n\t"
                    "th.vfmul.vf v13, v13, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v14, v13\n\t"
                    "th.vadd.vx v14, v14, %[exp_bias]\n\t"
                    "th.vfadd.vf v13, v14, %[one]\n\t"
                    "th.vfdiv.vv v12, v12, v13\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v17, v16\n\t"
                    "th.vfmax.vf v17, v17, %[clamp_min]\n\t"
                    "th.vfmin.vf v17, v17, %[clamp_max]\n\t"
                    "th.vfmul.vf v17, v17, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v18, v17\n\t"
                    "th.vadd.vx v18, v18, %[exp_bias]\n\t"
                    "th.vfadd.vf v17, v18, %[one]\n\t"
                    "th.vfdiv.vv v16, v16, v17\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vfneg.v v21, v20\n\t"
                    "th.vfmax.vf v21, v21, %[clamp_min]\n\t"
                    "th.vfmin.vf v21, v21, %[clamp_max]\n\t"
                    "th.vfmul.vf v21, v21, %[exp_alpha]\n\t"
                    "th.vfcvt.x.f.v v22, v21\n\t"
                    "th.vadd.vx v22, v22, %[exp_bias]\n\t"
                    "th.vfadd.vf v21, v22, %[one]\n\t"
                    "th.vfdiv.vv v20, v20, v21\n\t"
                    
                    // --- Store all chunks ---
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v8, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v12, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v16, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"
                    "th.vsetvli x0, %[vl], e32, m1\n\t"
                    "th.vse.v v20, (%[y_ptr])\n\t"
                    "add %[y_ptr], %[y_ptr], %[vl_bytes]\n\t"

                : [x_ptr] "+r"(x), [y_ptr] "+r"(y)
                : [vl] "r"(vl), [vl_bytes] "r"(vl_bytes),
                  [clamp_min] "f"(-87.3f), [clamp_max] "f"(88.7f),
                  [exp_alpha] "f"(exp_alpha_f), [exp_bias] "r"(exp_bias_i), [one] "f"(one_f)
                : "memory", "t0", "v10", "v12", "v13", "v14", "v16", "v17", "v18", "v20", "v21", "v22", "v8", "v9"            );
            i += vl * 4;
        } else {
            // Process remaining tail or single chunk
            process_silu_tail_asm_fast_ggml_vec_silu_f32_asm_unroll4_interleaved(y + i, x + i, vl);
            i += vl;
        }
    }
}
#endif
