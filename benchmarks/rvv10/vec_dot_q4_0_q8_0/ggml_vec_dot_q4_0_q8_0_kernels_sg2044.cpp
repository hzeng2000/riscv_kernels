// Auto-generated kernels for ggml_vec_dot_q4_0_q8_0 on sg2044
// RVV Version: rvv_1.0
// Generated by VectorWeaver

#include "ggml_vec_dot_q4_0_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_rvv_intrinsics
// Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_rvv_intrinsics(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    float sumf = 0.0f;

    for (int ib = 0; ib < nb; ++ib) {
        size_t vl = QK8_0 / 2; // 16 elements for nibble unpacking
        
        // Load packed q4 data
        vuint8m1_t q4_packed = __riscv_vle8_v_u8m1(x[ib].qs, vl);
        
        // Unpack low nibbles (lower 4 bits)
        vint8m1_t q4_lo = __riscv_vreinterpret_v_u8m1_i8m1(
            __riscv_vand_vx_u8m1(q4_packed, 0x0F, vl));
        // Unpack high nibbles (upper 4 bits)
        vint8m1_t q4_hi = __riscv_vreinterpret_v_u8m1_i8m1(
            __riscv_vsrl_vx_u8m1(q4_packed, 4, vl));
        
        // Load q8 data (two halves)
        vint8m1_t q8_lo = __riscv_vle8_v_i8m1(y[ib].qs, vl);
        vint8m1_t q8_hi = __riscv_vle8_v_i8m1(y[ib].qs + 16, vl);
        
        // Subtract 8 to get signed values (-8 to 7)
        q4_lo = __riscv_vsub_vx_i8m1(q4_lo, 8, vl);
        q4_hi = __riscv_vsub_vx_i8m1(q4_hi, 8, vl);
        
        // Widening multiply and accumulate
        vint16m2_t prod_lo = __riscv_vwmul_vv_i16m2(q4_lo, q8_lo, vl);
        vint16m2_t prod_hi = __riscv_vwmul_vv_i16m2(q4_hi, q8_hi, vl);
        vint16m2_t prod_sum = __riscv_vadd_vv_i16m2(prod_lo, prod_hi, vl);
        
        // Reduce to scalar
        vint32m1_t v_zero = __riscv_vmv_v_x_i32m1(0, vl);
        vint32m1_t v_sum = __riscv_vwredsum_vs_i16m2_i32m1(prod_sum, v_zero, vl);
        int32_t sumi = __riscv_vmv_x_s_i32m1_i32(v_sum);
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    *s = sumf;
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_baseline(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_baseline
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_baseline(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;


    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_baseline(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;


    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_unroll2(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;


    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            // Block 0
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vand.vi v9, v8, 15\n\t"
            "vsub.vx v9, v9, %[sub_val]\n\t"
            "vsrl.vi v8, v8, 4\n\t"
            "vsub.vx v8, v8, %[sub_val]\n\t"
            "vle8.v v10, (%[y0_lo_ptr])\n\t"
            "vle8.v v11, (%[y0_hi_ptr])\n\t"
            // Widening multiply (e8 -> e16)
            "vwmul.vv v12, v9, v10\n\t"
            "vwmul.vv v20, v8, v11\n\t"
            // Add the two products
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v12, v12, v20\n\t"
            // Initialize accumulator (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v14, x0\n\t"
            // Widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v14, v12, v14\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v16, (%[x1_ptr])\n\t"
            "vand.vi v17, v16, 15\n\t"
            "vsub.vx v17, v17, %[sub_val]\n\t"
            "vsrl.vi v16, v16, 4\n\t"
            "vsub.vx v16, v16, %[sub_val]\n\t"
            "vle8.v v18, (%[y1_lo_ptr])\n\t"
            "vle8.v v19, (%[y1_hi_ptr])\n\t"
            // Widening multiply (e8 -> e16)
            "vwmul.vv v20, v17, v18\n\t"
            "vwmul.vv v12, v16, v19\n\t"
            // Add the two products
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v20, v20, v12\n\t"
            // Initialize accumulator (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v22, x0\n\t"
            // Widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v22, v20, v22\n\t"
            // Store all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v14\n\t"
            "vmv.x.s %[sumi1], v22\n\t"
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  
              [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs),
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),  
              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs),
              [y1_hi_ptr] "r"(y[ib+1].qs + 16) ,
              [sub_val] "r"(8)
            :               "t0", "memory", "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;


    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            // Block 0
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vand.vi v9, v8, 15\n\t"
            "vsub.vx v9, v9, %[sub_val]\n\t"
            "vsrl.vi v8, v8, 4\n\t"
            "vsub.vx v8, v8, %[sub_val]\n\t"
            "vle8.v v10, (%[y0_lo_ptr])\n\t"
            "vle8.v v11, (%[y0_hi_ptr])\n\t"
            // Widening multiply (e8 -> e16)
            "vwmul.vv v12, v9, v10\n\t"
            "vwmul.vv v20, v8, v11\n\t"
            // Add the two products
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v12, v12, v20\n\t"
            // Initialize accumulator (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v14, x0\n\t"
            // Widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v14, v12, v14\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v16, (%[x1_ptr])\n\t"
            "vand.vi v17, v16, 15\n\t"
            "vsub.vx v17, v17, %[sub_val]\n\t"
            "vsrl.vi v16, v16, 4\n\t"
            "vsub.vx v16, v16, %[sub_val]\n\t"
            "vle8.v v18, (%[y1_lo_ptr])\n\t"
            "vle8.v v19, (%[y1_hi_ptr])\n\t"
            // Widening multiply (e8 -> e16)
            "vwmul.vv v20, v17, v18\n\t"
            "vwmul.vv v12, v16, v19\n\t"
            // Add the two products
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v20, v20, v12\n\t"
            // Initialize accumulator (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v22, x0\n\t"
            // Widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v22, v20, v22\n\t"
            // Store all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v14\n\t"
            "vmv.x.s %[sumi1], v22\n\t"
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  
              [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs),
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),  
              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs),
              [y1_hi_ptr] "r"(y[ib+1].qs + 16) ,
              [sub_val] "r"(8)
            :               "t0", "memory", "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;


    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            // 1. Load all data
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_lo_ptr])\n\t"
            "vle8.v v11, (%[y0_hi_ptr])\n\t"
            "vle8.v v16, (%[x1_ptr])\n\t"
            "vle8.v v18, (%[y1_lo_ptr])\n\t"
            "vle8.v v19, (%[y1_hi_ptr])\n\t"
            // 2. Unpack all q4 data
            "vand.vi v9, v8, 15\n\t"
            "vsub.vx v9, v9, %[sub_val]\n\t"
            "vsrl.vi v8, v8, 4\n\t"
            "vsub.vx v8, v8, %[sub_val]\n\t"
            "vand.vi v17, v16, 15\n\t"
            "vsub.vx v17, v17, %[sub_val]\n\t"
            "vsrl.vi v16, v16, 4\n\t"
            "vsub.vx v16, v16, %[sub_val]\n\t"
            // 3. Perform all multiplications (low parts)
            "vwmul.vv v12, v9, v10\n\t"
            "vwmul.vv v20, v17, v18\n\t"
            // 4. Multiply high parts (reuse reduce_acc as temp storage)
            "vwmul.vv v14, v8, v11\n\t"
            "vwmul.vv v22, v16, v19\n\t"
            // 5. Add low and high parts together
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v12, v12, v14\n\t"
            "vadd.vv v20, v20, v22\n\t"
            // 6. Perform all reductions
            // Initialize accumulators (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v14, x0\n\t"
            "vmv.s.x v22, x0\n\t"
            // Widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v14, v12, v14\n\t"
            "vwredsum.vs v22, v20, v22\n\t"
            // Store all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v14\n\t"
            "vmv.x.s %[sumi1], v22\n\t"
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  
              [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs),
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),  
              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs),
              [y1_hi_ptr] "r"(y[ib+1].qs + 16) ,
              [sub_val] "r"(8)
            :               "t0", "memory", "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;


    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 16\n\t"
            // 1. Load all data
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_lo_ptr])\n\t"
            "vle8.v v11, (%[y0_hi_ptr])\n\t"
            "vle8.v v16, (%[x1_ptr])\n\t"
            "vle8.v v18, (%[y1_lo_ptr])\n\t"
            "vle8.v v19, (%[y1_hi_ptr])\n\t"
            // 2. Unpack all q4 data
            "vand.vi v9, v8, 15\n\t"
            "vsub.vx v9, v9, %[sub_val]\n\t"
            "vsrl.vi v8, v8, 4\n\t"
            "vsub.vx v8, v8, %[sub_val]\n\t"
            "vand.vi v17, v16, 15\n\t"
            "vsub.vx v17, v17, %[sub_val]\n\t"
            "vsrl.vi v16, v16, 4\n\t"
            "vsub.vx v16, v16, %[sub_val]\n\t"
            // 3. Perform all multiplications (low parts)
            "vwmul.vv v12, v9, v10\n\t"
            "vwmul.vv v20, v17, v18\n\t"
            // 4. Multiply high parts (reuse reduce_acc as temp storage)
            "vwmul.vv v14, v8, v11\n\t"
            "vwmul.vv v22, v16, v19\n\t"
            // 5. Add low and high parts together
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v12, v12, v14\n\t"
            "vadd.vv v20, v20, v22\n\t"
            // 6. Perform all reductions
            // Initialize accumulators (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v14, x0\n\t"
            "vmv.s.x v22, x0\n\t"
            // Widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v14, v12, v14\n\t"
            "vwredsum.vs v22, v20, v22\n\t"
            // Store all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v14\n\t"
            "vmv.x.s %[sumi1], v22\n\t"
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  
              [x0_ptr] "r"(x[ib+0].qs), 
              [y0_lo_ptr] "r"(y[ib+0].qs),
              [y0_hi_ptr] "r"(y[ib+0].qs + 16),  
              [x1_ptr] "r"(x[ib+1].qs), 
              [y1_lo_ptr] "r"(y[ib+1].qs),
              [y1_hi_ptr] "r"(y[ib+1].qs + 16) ,
              [sub_val] "r"(8)
            :               "t0", "memory", "memory", "t0", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v8", "v9");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_batch4(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;


    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_batch4(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(const block_q4_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 16\n\t"
        "vsetvli x0, t0, e8, m1, ta, ma\n\t"
        // Load packed q4 (16 bytes = 32 nibbles)
        "vle8.v v8, (%[x_ptr])\n\t"
        // Unpack low nibbles
        "vand.vi v10, v8, 15\n\t"
        "vsub.vx v10, v10, %[sub_val]\n\t"  // subtract 8
        // Unpack high nibbles
        "vsrl.vi v11, v8, 4\n\t"
        "vsub.vx v11, v11, %[sub_val]\n\t"  // subtract 8
        // Load q8 halves
        "vle8.v v12, (%[y_lo_ptr])\n\t"
        "vle8.v v13, (%[y_hi_ptr])\n\t"
        // Widening multiply (e8 -> e16)
        "vwmul.vv v16, v10, v12\n\t"
        "vwmul.vv v18, v11, v13\n\t"
        // Add the two products (e16)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vadd.vv v16, v16, v18\n\t"
        // Initialize accumulator (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m2, ta, ma\n\t"
        "vwredsum.vs v24, v16, v24\n\t"
        // Extract result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), 
          [y_lo_ptr] "r"(y->qs), 
          [y_hi_ptr] "r"(y->qs + 16),
          [sub_val] "r"(8)
        : "t0", "memory", "v8", "v10", "v11", "v12", "v13", "v16", "v17", "v24"
    );
    *sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d));
}

// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0
void ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;


    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_q4_q8_asm_ggml_vec_dot_q4_0_q8_0_asm_f64_batch4(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: RVV 1.0, Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q4_0_q8_0_asm_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            "vmv.s.x v26, x0\n\t"
            "vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x0_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y0_lo])\n\t"
            "vle8.v v4, (%[y0_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x1_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y1_lo])\n\t"
            "vle8.v v4, (%[y1_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x2_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y2_lo])\n\t"
            "vle8.v v4, (%[y2_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x3_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y3_lo])\n\t"
            "vle8.v v4, (%[y3_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[s0], v24\n\t"
            "vmv.x.s %[s1], v25\n\t"
            "vmv.x.s %[s2], v26\n\t"
            "vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_ptr] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_ptr] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_ptr] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_ptr] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v24", "v25", "v26", "v27", "v3", "v4", "v8", "v9"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: RVV 1.0, Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            "vmv.s.x v26, x0\n\t"
            "vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x0_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y0_lo])\n\t"
            "vle8.v v4, (%[y0_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x1_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y1_lo])\n\t"
            "vle8.v v4, (%[y1_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x2_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y2_lo])\n\t"
            "vle8.v v4, (%[y2_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            // Load and unpack q4
            "vle8.v v0, (%[x3_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            // Load q8 halves
            "vle8.v v3, (%[y3_lo])\n\t"
            "vle8.v v4, (%[y3_hi])\n\t"
            // Widening multiply and accumulate
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            // Reduce
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[s0], v24\n\t"
            "vmv.x.s %[s1], v25\n\t"
            "vmv.x.s %[s2], v26\n\t"
            "vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_ptr] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [y0_hi] "r"(y[ib+0].qs + 16), [x1_ptr] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [y1_hi] "r"(y[ib+1].qs + 16), [x2_ptr] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [y2_hi] "r"(y[ib+2].qs + 16), [x3_ptr] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [y3_hi] "r"(y[ib+3].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v24", "v25", "v26", "v27", "v3", "v4", "v8", "v9"        );
        
        sumf += (float)sumi[0] * (GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d));
        sumf += (float)sumi[1] * (GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d));
        sumf += (float)sumi[2] * (GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d));
        sumf += (float)sumi[3] * (GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d));
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            "vwmul.vv v8, v1, v3\n\t"
            "vwmacc.vv v8, v2, v4\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v16"
        );
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll2_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_unroll4_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q4_0_q8_0_asm_f64_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q4_0 *GGML_RESTRICT x = (const block_q4_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and unpack q4
            "vle8.v v0, (%[x_ptr])\n\t"
            "vand.vi v1, v0, 15\n\t"
            "vsub.vx v1, v1, %[sub_val]\n\t"
            "vsrl.vi v2, v0, 4\n\t"
            "vsub.vx v2, v2, %[sub_val]\n\t"
            
            // Load q8 halves
            "vle8.v v3, (%[y_lo])\n\t"
            "vle8.v v4, (%[y_hi])\n\t"
            
            // Widening multiply (separate)
            "vwmul.vv v8, v1, v3\n\t"
            "vwmul.vv v10, v2, v4\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v8, v8, v10\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            
            : [sumi] "=r"(sumi)
            : [x_ptr] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs), [y_hi] "r"(y[ib].qs + 16),
              [sub_val] "r"(8)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v8", "v9", "v10", "v11", "v16"
        );
        
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    
    *s = sumf;
}
#endif


