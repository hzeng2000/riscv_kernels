// Auto-generated kernels for ggml_vec_dot_q8_0_q8_0 on sg2044
// RVV Version: rvv_1.0
// Generated by VectorWeaver

#include "ggml_vec_dot_q8_0_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_rvv_intrinsics
// Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_rvv_intrinsics(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    float sumf = 0.0f;
    for (int ib = 0; ib < nb; ++ib) {
        size_t vl = QK8_0;
        vint8m2_t bx_0 = __riscv_vle8_v_i8m2(x[ib].qs, vl);
        vint8m2_t by_0 = __riscv_vle8_v_i8m2(y[ib].qs, vl);
        vint16m4_t vw_mul = __riscv_vwmul_vv_i16m4(bx_0, by_0, vl);
        vint32m1_t v_zero = __riscv_vmv_v_x_i32m1(0, vl);
        vint32m1_t v_sum = __riscv_vwredsum_vs_i16m4_i32m1(vw_mul, v_zero, vl);
        int32_t sumi = __riscv_vmv_x_s_i32m1_i32(v_sum);
        sumf += (float)sumi * (GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d));
    }
    *s = sumf;
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[x1_ptr])\n\t"
            "vle8.v v12, (%[y0_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // Perform widening multiplications: int8 -> int16
            "vwmul.vv v16, v8, v12\n\t"
            "vwmul.vv v20, v10, v14\n\t"
            // Initialize accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v28, x0\n\t"
            // Perform widening reductions: int16 -> int32
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v28, v20, v28\n\t"
            // Extract scalar results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v28\n\t"

            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=True, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[x1_ptr])\n\t"
            "vle8.v v12, (%[y0_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // Perform widening multiplications: int8 -> int16
            "vwmul.vv v16, v8, v12\n\t"
            "vwmul.vv v20, v10, v14\n\t"
            // Initialize accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v28, x0\n\t"
            // Perform widening reductions: int16 -> int32
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v28, v20, v28\n\t"
            // Extract scalar results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v28\n\t"

            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_fused_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_baseline(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_baseline
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_baseline(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v12, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v16, v12, v16\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_baseline(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v12, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v16, v12, v16\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v16, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            "vwmul.vv v16, v12, v14\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v25, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v25, v16, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v16, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            "vwmul.vv v16, v12, v14\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v25, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v25, v16, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v26, (%[x_pf0_ptr])\n\t"
            "vle8.v v28, (%[y_pf0_ptr])\n\t"

            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v16, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            "vwmul.vv v16, v12, v14\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v25, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v25, v16, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v26, (%[x_pf0_ptr])\n\t"
            "vle8.v v28, (%[y_pf0_ptr])\n\t"

            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v16, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            // Block 1
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            "vwmul.vv v16, v12, v14\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v25, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v25, v16, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // 1. Load all data for current compute window
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "vwmul.vv v16, v8, v10\n\t"
            "vwmul.vv v20, v12, v14\n\t"
            // 3. Initialize all accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            // 4. Perform all widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v25, v20, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 1 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // 1. Load all data for current compute window
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "vwmul.vv v16, v8, v10\n\t"
            "vwmul.vv v20, v12, v14\n\t"
            // 3. Initialize all accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            // 4. Perform all widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v25, v20, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v26, (%[x_pf0_ptr])\n\t"
            "vle8.v v28, (%[y_pf0_ptr])\n\t"

            // 1. Load all data for current compute window
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "vwmul.vv v16, v8, v10\n\t"
            "vwmul.vv v20, v12, v14\n\t"
            // 3. Initialize all accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            // 4. Perform all widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v25, v20, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 2 < nb; ib += 2) {
        int32_t sumi[2];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/

"vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v26, (%[x_pf0_ptr])\n\t"
            "vle8.v v28, (%[y_pf0_ptr])\n\t"

            // 1. Load all data for current compute window
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vle8.v v12, (%[x1_ptr])\n\t"
            "vle8.v v14, (%[y1_ptr])\n\t"
            // 2. Perform all multiplications
            "vwmul.vv v16, v8, v10\n\t"
            "vwmul.vv v20, v12, v14\n\t"
            // 3. Initialize all accumulators with correct SEW (e32)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            // 4. Perform all widening reductions (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v24, v16, v24\n\t"
            "vwredsum.vs v25, v20, v25\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v24\n\t"
            "vmv.x.s %[sumi1], v25\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0]),  [sumi1] "=r"(sumi[1])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs),  [x1_ptr] "r"(x[ib+1].qs), [y1_ptr] "r"(y[ib+1].qs),  [x_pf0_ptr] "r"(x[ib+2].qs), [y_pf0_ptr] "r"(y[ib+2].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sum_d += (double)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v33, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v43, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v47, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v51, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_batch4(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v12, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v16, v12, v16\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_batch4(&x[ib], &y[ib], &sumf);
    }
    *s = sumf;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(const block_q8_0* x, const block_q8_0* y, float* sumf) {
    int32_t sumi;
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e8, m2, ta, ma\n\t"
        "vle8.v v8, (%[x_ptr])\n\t"
        "vle8.v v10, (%[y_ptr])\n\t"
        "vwmul.vv v12, v8, v10\n\t"
        // Initialize accumulator with correct SEW (e32)
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.s.x v24, x0\n\t"
        // Perform widening reduction (e16 -> e32)
        "vsetvli x0, t0, e16, m4, ta, ma\n\t"
        "vwredsum.vs v24, v12, v24\n\t"
        // Extract scalar result
        "vsetvli x0, t0, e32, m1, ta, ma\n\t"
        "vmv.x.s %[sumi], v24\n\t"
        : [sumi] "=r"(sumi)
        : [x_ptr] "r"(x->qs), [y_ptr] "r"(y->qs)
        : "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v24"
    );
    *sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x->d) * GGML_CPU_FP16_TO_FP32(y->d);
}

// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='full', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0
void ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    double sum_d = 0.0;
    int ib = 0;

 
    for (; ib + 0 < nb; ib += 1) {
        int32_t sumi[1];
        asm volatile(
            "li t0, 32\n\t"
            /******************* CODE BLOCK START *******************/


            // Block 0
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vle8.v v8, (%[x0_ptr])\n\t"
            "vle8.v v10, (%[y0_ptr])\n\t"
            "vwmul.vv v12, v8, v10\n\t"
            // Initialize accumulator with correct SEW (e32 for widened result)
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            // Perform widening reduction (e16 -> e32)
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vwredsum.vs v16, v12, v16\n\t"

            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi0], v16\n\t"
            /******************** CODE BLOCK END ********************/
            :  [sumi0] "=r"(sumi[0])             :  [x0_ptr] "r"(x[ib+0].qs), [y0_ptr] "r"(y[ib+0].qs)             :               "t0", "memory", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16");
        
        sum_d += (double)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
    }

    // Tail loop
    float temp_sumf = 0.0f;
    for (; ib < nb; ++ib) {
        process_single_block_asm_ggml_vec_dot_q8_0_q8_0_asm_f64_batch4(&x[ib], &y[ib], &temp_sumf);
    }
    sum_d += temp_sumf;
    *s = (float)sum_d;
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split_macc
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=1)
// Target: RVV 1.0, Split LMUL + Fused MACC
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split_macc(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and fused multiply-accumulate second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmacc.vv v2, v4, v5\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: RVV 1.0, Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q8_0_q8_0_asm_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            "vmv.s.x v26, x0\n\t"
            "vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x0_lo])\n\t"
            "vle8.v v1, (%[y0_lo])\n\t"
            "vle8.v v2, (%[x0_hi])\n\t"
            "vle8.v v3, (%[y0_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x1_lo])\n\t"
            "vle8.v v1, (%[y1_lo])\n\t"
            "vle8.v v2, (%[x1_hi])\n\t"
            "vle8.v v3, (%[y1_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x2_lo])\n\t"
            "vle8.v v1, (%[y2_lo])\n\t"
            "vle8.v v2, (%[x2_hi])\n\t"
            "vle8.v v3, (%[y2_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x3_lo])\n\t"
            "vle8.v v1, (%[y3_lo])\n\t"
            "vle8.v v2, (%[x3_hi])\n\t"
            "vle8.v v3, (%[y3_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[s0], v24\n\t"
            "vmv.x.s %[s1], v25\n\t"
            "vmv.x.s %[s2], v26\n\t"
            "vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_lo] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [x0_hi] "r"(x[ib+0].qs + 16), [y0_hi] "r"(y[ib+0].qs + 16), [x1_lo] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [x1_hi] "r"(x[ib+1].qs + 16), [y1_hi] "r"(y[ib+1].qs + 16), [x2_lo] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [x2_hi] "r"(x[ib+2].qs + 16), [y2_hi] "r"(y[ib+2].qs + 16), [x3_lo] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [x3_hi] "r"(x[ib+3].qs + 16), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16", "v24", "v25", "v26", "v27"        );
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
        sumf += (float)sumi[2] * GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d);
        sumf += (float)sumi[3] * GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d);
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x_lo])\n\t"
            "vle8.v v1, (%[y_lo])\n\t"
            "vle8.v v2, (%[x_hi])\n\t"
            "vle8.v v3, (%[y_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs),
              [x_hi] "r"(x[ib].qs + 16), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=True, block_batch_size=4)
// Target: RVV 1.0, Split LMUL + Fused MACC + Batch4
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_macc_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    int ib = 0;
    
    const int BATCH_SIZE = 4;
    
    for (; ib + BATCH_SIZE <= nb; ib += BATCH_SIZE) {
        int32_t sumi[BATCH_SIZE];
        
        asm volatile(
            "li t0, 16\n\t"
            
            // Initialize accumulators to zero
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v24, x0\n\t"
            "vmv.s.x v25, x0\n\t"
            "vmv.s.x v26, x0\n\t"
            "vmv.s.x v27, x0\n\t"
            
            // ===== Block 0 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x0_lo])\n\t"
            "vle8.v v1, (%[y0_lo])\n\t"
            "vle8.v v2, (%[x0_hi])\n\t"
            "vle8.v v3, (%[y0_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v24, v8, v24\n\t"
            // ===== Block 1 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x1_lo])\n\t"
            "vle8.v v1, (%[y1_lo])\n\t"
            "vle8.v v2, (%[x1_hi])\n\t"
            "vle8.v v3, (%[y1_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v25, v8, v25\n\t"
            // ===== Block 2 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x2_lo])\n\t"
            "vle8.v v1, (%[y2_lo])\n\t"
            "vle8.v v2, (%[x2_hi])\n\t"
            "vle8.v v3, (%[y2_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v26, v8, v26\n\t"
            // ===== Block 3 =====
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x3_lo])\n\t"
            "vle8.v v1, (%[y3_lo])\n\t"
            "vle8.v v2, (%[x3_hi])\n\t"
            "vle8.v v3, (%[y3_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v27, v8, v27\n\t"
            
            // Extract all results
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[s0], v24\n\t"
            "vmv.x.s %[s1], v25\n\t"
            "vmv.x.s %[s2], v26\n\t"
            "vmv.x.s %[s3], v27\n\t"
            
            : [s0] "=r"(sumi[0]), [s1] "=r"(sumi[1]), [s2] "=r"(sumi[2]), [s3] "=r"(sumi[3])            : [x0_lo] "r"(x[ib+0].qs), [y0_lo] "r"(y[ib+0].qs), [x0_hi] "r"(x[ib+0].qs + 16), [y0_hi] "r"(y[ib+0].qs + 16), [x1_lo] "r"(x[ib+1].qs), [y1_lo] "r"(y[ib+1].qs), [x1_hi] "r"(x[ib+1].qs + 16), [y1_hi] "r"(y[ib+1].qs + 16), [x2_lo] "r"(x[ib+2].qs), [y2_lo] "r"(y[ib+2].qs), [x2_hi] "r"(x[ib+2].qs + 16), [y2_hi] "r"(y[ib+2].qs + 16), [x3_lo] "r"(x[ib+3].qs), [y3_lo] "r"(y[ib+3].qs), [x3_hi] "r"(x[ib+3].qs + 16), [y3_hi] "r"(y[ib+3].qs + 16)            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16", "v24", "v25", "v26", "v27"        );
        
        sumf += (float)sumi[0] * GGML_CPU_FP16_TO_FP32(x[ib+0].d) * GGML_CPU_FP16_TO_FP32(y[ib+0].d);
        sumf += (float)sumi[1] * GGML_CPU_FP16_TO_FP32(x[ib+1].d) * GGML_CPU_FP16_TO_FP32(y[ib+1].d);
        sumf += (float)sumi[2] * GGML_CPU_FP16_TO_FP32(x[ib+2].d) * GGML_CPU_FP16_TO_FP32(y[ib+2].d);
        sumf += (float)sumi[3] * GGML_CPU_FP16_TO_FP32(x[ib+3].d) * GGML_CPU_FP16_TO_FP32(y[ib+3].d);
    }
    
    // Tail loop
    for (; ib < nb; ++ib) {
        int32_t sumi;
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            "vle8.v v0, (%[x_lo])\n\t"
            "vle8.v v1, (%[y_lo])\n\t"
            "vle8.v v2, (%[x_hi])\n\t"
            "vle8.v v3, (%[y_hi])\n\t"
            "vwmul.vv v8, v0, v1\n\t"
            "vwmacc.vv v8, v2, v3\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v16, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v16, v8, v16\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v16\n\t"
            : [sumi] "=r"(sumi)
            : [x_lo] "r"(x[ib].qs), [y_lo] "r"(y[ib].qs),
              [x_hi] "r"(x[ib].qs + 16), [y_hi] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v8", "v9", "v16"
        );
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll2_interleaved_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=1, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch1_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=2, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_unroll4_interleaved_prefetch2_f64_split(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif




/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
 * -----------------------------------------------------------------------------
 */



#if defined(__RVV_ASM_STD) || defined(__riscv_v)
// Generated function: ggml_vec_dot_q8_0_q8_0_asm_f64_split_batch4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f64', lmul_strategy='split', use_fused_macc=False, block_batch_size=4)
// Target: RVV 1.0, Split LMUL
void ggml_vec_dot_q8_0_q8_0_asm_f64_split_batch4(int n, float *s, const void *vx, const void *vy) {
    const int nb = n / QK8_0;
    const block_q8_0 *GGML_RESTRICT x = (const block_q8_0 *)vx;
    const block_q8_0 *GGML_RESTRICT y = (const block_q8_0 *)vy;
    
    float sumf = 0.0f;
    
    for (int ib = 0; ib < nb; ++ib) {
        int32_t sumi;
        
        asm volatile(
            "li t0, 16\n\t"
            "vsetvli x0, t0, e8, m1, ta, ma\n\t"
            
            // Load and multiply first half
            "vle8.v v0, (%[x0])\n\t"
            "vle8.v v1, (%[y0])\n\t"
            "vwmul.vv v2, v0, v1\n\t"
            
            // Load and multiply second half
            "vle8.v v4, (%[x1])\n\t"
            "vle8.v v5, (%[y1])\n\t"
            "vwmul.vv v6, v4, v5\n\t"
            
            // Add both halves
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vadd.vv v2, v2, v6\n\t"
            
            // Reduce
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.s.x v8, x0\n\t"
            "vsetvli x0, t0, e16, m2, ta, ma\n\t"
            "vwredsum.vs v8, v2, v8\n\t"
            
            "vsetvli x0, t0, e32, m1, ta, ma\n\t"
            "vmv.x.s %[sumi], v8\n\t"
            
            : [sumi] "=r"(sumi)
            : [x0] "r"(x[ib].qs), [y0] "r"(y[ib].qs),
              [x1] "r"(x[ib].qs + 16), [y1] "r"(y[ib].qs + 16)
            : "t0", "memory", "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8"
        );
        
        sumf += (float)sumi * GGML_CPU_FP16_TO_FP32(x[ib].d) * GGML_CPU_FP16_TO_FP32(y[ib].d);
    }
    
    *s = sumf;
}
#endif


