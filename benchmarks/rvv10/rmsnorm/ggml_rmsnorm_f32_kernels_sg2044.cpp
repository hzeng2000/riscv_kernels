// Auto-generated kernels for ggml_rmsnorm_f32 on sg2044
// RVV Version: rvv_1.0
// Generated by VectorWeaver

#include "ggml_rmsnorm_f32_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__riscv_v)
// Generated function: ggml_rmsnorm_f32_rvv_intrinsics
// Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_rvv_intrinsics(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f = 0.0f;
    size_t vl;
    vfloat32m8_t v_acc_sq = __riscv_vfmv_v_f_f32m8(0.0f, __riscv_vsetvl_e32m8(n));
    for (int i = 0; i < n; i += vl) {
        vl = __riscv_vsetvl_e32m8(n - i);
        vfloat32m8_t v_x = __riscv_vle32_v_f32m8(x + i, vl);
        v_acc_sq = __riscv_vfmacc_vv_f32m8(v_acc_sq, v_x, v_x, vl);
    }
    vfloat32m1_t v_zero = __riscv_vfmv_v_f_f32m1(0.0f, 1);
    vfloat32m1_t v_sum_sq_vec = __riscv_vfredosum_vs_f32m8_f32m1(v_acc_sq, v_zero, __riscv_vsetvl_e32m8(n));
    sum_sq_f = __riscv_vfmv_f_s_f32m1_f32(v_sum_sq_vec);
    const float mean = sum_sq_f / n;
    const float scale = 1.0f / sqrtf(mean + eps);
    for (int i = 0; i < n; i += vl) {
        vl = __riscv_vsetvl_e32m8(n - i);
        vfloat32m8_t v_x = __riscv_vle32_v_f32m8(x + i, vl);
        vfloat32m8_t v_y = __riscv_vfmul_vf_f32m8(v_x, scale, vl);
        __riscv_vse32_v_f32m8(y + i, v_y, vl);
    }
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Generated function: ggml_rmsnorm_f32_baseline
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_baseline(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f;
    const int n_total = n; // Keep original n for mean calculation
    const float *x_orig = x; // Keep original x pointer for Pass 2

    // ================== Pass 1: Sum of Squares ==================
    asm volatile (
        "vsetvli x0, x0, e32, m2, ta, ma\n\t"
"vmv.v.i v8, 0\n\t"        // --- Tail Loop ---
"1:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 2f\n\t"
        "slli t2, t0, 2\n\t"
        "vle32.v v0, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmacc.vv v8, v0, v0\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 1b\n\t"
        // --- Reduction ---
"2:\n\t"        // Reduce to scalar
        "vsetvli x0, x0, e32, m1, ta, ma\n\t"
        "vmv.v.i v24, 0\n\t"
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
        "vfredosum.vs v24, v8, v24\n\t"
        "vfmv.f.s %[sum_out], v24\n\t"
        : [sum_out] "=f"(sum_sq_f), [x_ptr] "+r"(x), [n_in] "+r"(n)
        : [n_total] "r"(n_total)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );

    const float mean = sum_sq_f / (float)n_total;
    const float scale = 1.0f / sqrtf(mean + eps);

    // ================== Pass 2: Apply Scale ==================
    // CRITICAL: We need new local copies for pass 2, initialized with original values.
    const float *x_ptr_pass2 = x_orig; // Use saved x pointer (x was modified by Pass 1)
    float *y_ptr_pass2 = y;
    int n_pass2 = n_total; // Use the saved n_total (n was modified by Pass 1)
    asm volatile(
        "flw fa0, 0(%[scale_ptr])\n\t"
        // --- Tail Loop ---
"1:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 2f\n\t"
        "vle32.v v8, (%[x_ptr])\n\t"
        "slli t2, t0, 2\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 1b\n\t"
"2:\n\t"        : [x_ptr] "+r"(x_ptr_pass2), [y_ptr] "+r"(y_ptr_pass2), [n_in] "+r"(n_pass2)
        : [scale_ptr] "r"(&scale)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Generated function: ggml_rmsnorm_f32_asm_unroll2
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_asm_unroll2(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f;
    const int n_total = n; // Keep original n for mean calculation
    const float *x_orig = x; // Keep original x pointer for Pass 2

    // ================== Pass 1: Sum of Squares ==================
    asm volatile (
        "vsetvli x0, x0, e32, m2, ta, ma\n\t"
"vmv.v.i v8, 0\n\t""vmv.v.i v12, 0\n\t"        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 1\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
            "vle32.v v0, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v8, v0, v0\n\t"
            "vle32.v v2, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v12, v2, v2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "slli t2, t0, 2\n\t"
        "vle32.v v0, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmacc.vv v8, v0, v0\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
        // --- Reduction ---
"3:\n\t"        // Add multiple accumulators together (keep m2 LMUL)
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
"vfadd.vv v8, v8, v12\n\t"        // Reduce to scalar
        "vsetvli x0, x0, e32, m1, ta, ma\n\t"
        "vmv.v.i v24, 0\n\t"
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
        "vfredosum.vs v24, v8, v24\n\t"
        "vfmv.f.s %[sum_out], v24\n\t"
        : [sum_out] "=f"(sum_sq_f), [x_ptr] "+r"(x), [n_in] "+r"(n)
        : [n_total] "r"(n_total)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );

    const float mean = sum_sq_f / (float)n_total;
    const float scale = 1.0f / sqrtf(mean + eps);

    // ================== Pass 2: Apply Scale ==================
    // CRITICAL: We need new local copies for pass 2, initialized with original values.
    const float *x_ptr_pass2 = x_orig; // Use saved x pointer (x was modified by Pass 1)
    float *y_ptr_pass2 = y;
    int n_pass2 = n_total; // Use the saved n_total (n was modified by Pass 1)
    asm volatile(
        "flw fa0, 0(%[scale_ptr])\n\t"
        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 1\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
        "vle32.v v8, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v10, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v10, v10, fa0\n\t"
        "vse32.v v10, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "vle32.v v8, (%[x_ptr])\n\t"
        "slli t2, t0, 2\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
"3:\n\t"        : [x_ptr] "+r"(x_ptr_pass2), [y_ptr] "+r"(y_ptr_pass2), [n_in] "+r"(n_pass2)
        : [scale_ptr] "r"(&scale)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Generated function: ggml_rmsnorm_f32_asm_unroll2_interleaved
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_asm_unroll2_interleaved(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f;
    const int n_total = n; // Keep original n for mean calculation
    const float *x_orig = x; // Keep original x pointer for Pass 2

    // ================== Pass 1: Sum of Squares ==================
    asm volatile (
        "vsetvli x0, x0, e32, m2, ta, ma\n\t"
"vmv.v.i v8, 0\n\t"        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 1\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
            "vle32.v v0, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vle32.v v2, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v8, v0, v0\n\t"
            "vfmacc.vv v8, v2, v2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "slli t2, t0, 2\n\t"
        "vle32.v v0, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmacc.vv v8, v0, v0\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
        // --- Reduction ---
"3:\n\t"        // Reduce to scalar
        "vsetvli x0, x0, e32, m1, ta, ma\n\t"
        "vmv.v.i v24, 0\n\t"
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
        "vfredosum.vs v24, v8, v24\n\t"
        "vfmv.f.s %[sum_out], v24\n\t"
        : [sum_out] "=f"(sum_sq_f), [x_ptr] "+r"(x), [n_in] "+r"(n)
        : [n_total] "r"(n_total)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );

    const float mean = sum_sq_f / (float)n_total;
    const float scale = 1.0f / sqrtf(mean + eps);

    // ================== Pass 2: Apply Scale ==================
    // CRITICAL: We need new local copies for pass 2, initialized with original values.
    const float *x_ptr_pass2 = x_orig; // Use saved x pointer (x was modified by Pass 1)
    float *y_ptr_pass2 = y;
    int n_pass2 = n_total; // Use the saved n_total (n was modified by Pass 1)
    asm volatile(
        "flw fa0, 0(%[scale_ptr])\n\t"
        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 1\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
        "vle32.v v8, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v10, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v10, v10, fa0\n\t"
        "vse32.v v10, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "vle32.v v8, (%[x_ptr])\n\t"
        "slli t2, t0, 2\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
"3:\n\t"        : [x_ptr] "+r"(x_ptr_pass2), [y_ptr] "+r"(y_ptr_pass2), [n_in] "+r"(n_pass2)
        : [scale_ptr] "r"(&scale)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Generated function: ggml_rmsnorm_f32_asm_unroll4
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_asm_unroll4(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f;
    const int n_total = n; // Keep original n for mean calculation
    const float *x_orig = x; // Keep original x pointer for Pass 2

    // ================== Pass 1: Sum of Squares ==================
    asm volatile (
        "vsetvli x0, x0, e32, m2, ta, ma\n\t"
"vmv.v.i v8, 0\n\t""vmv.v.i v12, 0\n\t""vmv.v.i v16, 0\n\t""vmv.v.i v20, 0\n\t"        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 2\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
            "vle32.v v0, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v8, v0, v0\n\t"
            "vle32.v v2, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v12, v2, v2\n\t"
            "vle32.v v4, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v16, v4, v4\n\t"
            "vle32.v v6, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v20, v6, v6\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "slli t2, t0, 2\n\t"
        "vle32.v v0, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmacc.vv v8, v0, v0\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
        // --- Reduction ---
"3:\n\t"        // Add multiple accumulators together (keep m2 LMUL)
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
"vfadd.vv v8, v8, v12\n\t""vfadd.vv v8, v8, v16\n\t""vfadd.vv v8, v8, v20\n\t"        // Reduce to scalar
        "vsetvli x0, x0, e32, m1, ta, ma\n\t"
        "vmv.v.i v24, 0\n\t"
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
        "vfredosum.vs v24, v8, v24\n\t"
        "vfmv.f.s %[sum_out], v24\n\t"
        : [sum_out] "=f"(sum_sq_f), [x_ptr] "+r"(x), [n_in] "+r"(n)
        : [n_total] "r"(n_total)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );

    const float mean = sum_sq_f / (float)n_total;
    const float scale = 1.0f / sqrtf(mean + eps);

    // ================== Pass 2: Apply Scale ==================
    // CRITICAL: We need new local copies for pass 2, initialized with original values.
    const float *x_ptr_pass2 = x_orig; // Use saved x pointer (x was modified by Pass 1)
    float *y_ptr_pass2 = y;
    int n_pass2 = n_total; // Use the saved n_total (n was modified by Pass 1)
    asm volatile(
        "flw fa0, 0(%[scale_ptr])\n\t"
        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 2\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
        "vle32.v v8, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v10, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v10, v10, fa0\n\t"
        "vse32.v v10, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v12, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v12, v12, fa0\n\t"
        "vse32.v v12, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v14, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v14, v14, fa0\n\t"
        "vse32.v v14, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "vle32.v v8, (%[x_ptr])\n\t"
        "slli t2, t0, 2\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
"3:\n\t"        : [x_ptr] "+r"(x_ptr_pass2), [y_ptr] "+r"(y_ptr_pass2), [n_in] "+r"(n_pass2)
        : [scale_ptr] "r"(&scale)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */


#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Generated function: ggml_rmsnorm_f32_asm_unroll4_interleaved
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='interleaved', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void ggml_rmsnorm_f32_asm_unroll4_interleaved(int n, float *y, const float *x, float eps) {
    assert(n > 0);
    float sum_sq_f;
    const int n_total = n; // Keep original n for mean calculation
    const float *x_orig = x; // Keep original x pointer for Pass 2

    // ================== Pass 1: Sum of Squares ==================
    asm volatile (
        "vsetvli x0, x0, e32, m2, ta, ma\n\t"
"vmv.v.i v8, 0\n\t""vmv.v.i v12, 0\n\t""vmv.v.i v16, 0\n\t""vmv.v.i v20, 0\n\t"        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 2\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
            "vle32.v v0, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vle32.v v2, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vle32.v v4, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vle32.v v6, (%[x_ptr])\n\t"
            "add %[x_ptr], %[x_ptr], t2\n\t"
            "vfmacc.vv v8, v0, v0\n\t"
            "vfmacc.vv v12, v2, v2\n\t"
            "vfmacc.vv v16, v4, v4\n\t"
            "vfmacc.vv v20, v6, v6\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "slli t2, t0, 2\n\t"
        "vle32.v v0, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmacc.vv v8, v0, v0\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
        // --- Reduction ---
"3:\n\t"        // Add multiple accumulators together (keep m2 LMUL)
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
"vfadd.vv v8, v8, v12\n\t""vfadd.vv v8, v8, v16\n\t""vfadd.vv v8, v8, v20\n\t"        // Reduce to scalar
        "vsetvli x0, x0, e32, m1, ta, ma\n\t"
        "vmv.v.i v24, 0\n\t"
        "vsetvli x0, %[n_total], e32, m2, ta, ma\n\t"
        "vfredosum.vs v24, v8, v24\n\t"
        "vfmv.f.s %[sum_out], v24\n\t"
        : [sum_out] "=f"(sum_sq_f), [x_ptr] "+r"(x), [n_in] "+r"(n)
        : [n_total] "r"(n_total)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );

    const float mean = sum_sq_f / (float)n_total;
    const float scale = 1.0f / sqrtf(mean + eps);

    // ================== Pass 2: Apply Scale ==================
    // CRITICAL: We need new local copies for pass 2, initialized with original values.
    const float *x_ptr_pass2 = x_orig; // Use saved x pointer (x was modified by Pass 1)
    float *y_ptr_pass2 = y;
    int n_pass2 = n_total; // Use the saved n_total (n was modified by Pass 1)
    asm volatile(
        "flw fa0, 0(%[scale_ptr])\n\t"
        // Main loop for unroll > 1
        "1:\n\t"
        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "slli t1, t0, 2\n\t"
        "slt t3, %[n_in], t1\n\t"
        "bnez t3, 2f\n\t"
        "slli t2, t0, 2\n\t"
        // --- Main Loop Body ---
        "vle32.v v8, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v10, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v10, v10, fa0\n\t"
        "vse32.v v10, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v12, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v12, v12, fa0\n\t"
        "vse32.v v12, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "vle32.v v14, (%[x_ptr])\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v14, v14, fa0\n\t"
        "vse32.v v14, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t1\n\t"
        "bnez %[n_in], 1b\n\t"
        "j 3f\n\t"
        // --- Tail Loop ---
"2:\n\t"        "vsetvli t0, %[n_in], e32, m2, ta, ma\n\t"
        "beqz t0, 3f\n\t"
        "vle32.v v8, (%[x_ptr])\n\t"
        "slli t2, t0, 2\n\t"
        "add %[x_ptr], %[x_ptr], t2\n\t"
        "vfmul.vf v8, v8, fa0\n\t"
        "vse32.v v8, (%[y_ptr])\n\t"
        "add %[y_ptr], %[y_ptr], t2\n\t"
        "sub %[n_in], %[n_in], t0\n\t"
        "bnez %[n_in], 2b\n\t"
"3:\n\t"        : [x_ptr] "+r"(x_ptr_pass2), [y_ptr] "+r"(y_ptr_pass2), [n_in] "+r"(n_pass2)
        : [scale_ptr] "r"(&scale)
        : "v0", "t0", "v1", "t1", "v2", "t2", "t3", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "fa0", "memory"    );
}
#endif
