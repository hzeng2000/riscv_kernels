// Auto-generated kernels for quantize_row_q8_0 on sg2044
// RVV Version: rvv_1.0
// Generated by VectorWeaver

#include "quantize_row_q8_0_kernels.h"

#if defined(__riscv_v) || defined(__riscv_xtheadvector)
#include <riscv_vector.h>
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__riscv_v)
// Generated function: quantize_row_q8_0_rvv_intrinsics
// Config: TuningConfig(use_intrinsics=True, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void quantize_row_q8_0_rvv_intrinsics(const float * GGML_RESTRICT x, block_q8_0 * GGML_RESTRICT y, int64_t k) {
    size_t vl = QK8_0;
    const int nb = k / QK8_0;

    for (int i = 0; i < nb; i++) {
        vfloat32m8_t v_x   = __riscv_vle32_v_f32m8(x+i*QK8_0, vl);
        vfloat32m8_t vfabs = __riscv_vfabs_v_f32m8(v_x, vl);
        vfloat32m1_t tmp   = __riscv_vfmv_v_f_f32m1(0.0f, vl);
        vfloat32m1_t vmax  = __riscv_vfredmax_vs_f32m8_f32m1(vfabs, tmp, vl);
        float amax = __riscv_vfmv_f_s_f32m1_f32(vmax);

        const float d = amax / 127.0f;
        const float id = d ? 1.0f/d : 0.0f;
        y[i].d = GGML_FP32_TO_FP16(d);

        vfloat32m8_t x0 = __riscv_vfmul_vf_f32m8(v_x, id, vl);
        vint16m4_t   vi = __riscv_vfncvt_x_f_w_i16m4(x0, vl);
        vint8m2_t    vs = __riscv_vncvt_x_x_w_i8m2(vi, vl);
        __riscv_vse8_v_i8m2(y[i].qs , vs, vl);
    }
}
#endif



/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void quantize_row_q8_0_rvv_asm_one_block_quantize_row_q8_0_baseline(const float * GGML_RESTRICT x_ptr, block_q8_0 * GGML_RESTRICT y) {
    float amax;
    // --- Part 1: Calculate amax using RVV ---
    asm volatile (
        "li t0, 32\n\t"
        "vsetvli x0, t0, e32, m1, ta, ma\n\t" 
        "vmv.v.i v1, 0\n\t"

        "vsetvli x0, t0, e32, m8, ta, ma\n\t" 
        "vle32.v v8, (%[x_ptr])\n\t"
        "vfabs.v v8, v8\n\t" 

        "vfredmax.vs v8, v8, v1\n\t"

        "vfmv.f.s %[amax], v8\n\t"
        : [amax] "=f" (amax) : [x_ptr] "r" (x_ptr)
        : "t0", "memory", "v1", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15"
    );
    // --- C++ part: Calculate scaling factor ---
    float d = amax / 127.0f;
    float id = d ? 1.0f / d : 0.0f;
    y->d = GGML_CPU_FP32_TO_FP16(d);
    // --- Part 2: Scale, convert, and store using RVV ---
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e32, m8, ta, ma\n\t" 
        "vle32.v v8, (%[x_ptr])\n\t"
        "vfmul.vf v8, v8, %[id]\n\t"

        "vsetvli x0, t0, e16, m4, ta, ma\n\t" 
        "vfncvt.x.f.w v4, v8\n\t"

        "vsetvli x0, t0, e8, m2, ta, ma\n\t" 
        "vncvt.x.x.w v2, v4\n\t"

        "vse8.v v2, (%[y_ptr])\n\t"
        : : [x_ptr] "r" (x_ptr), [y_ptr] "r" (y->qs), [id] "f" (id)
        : "t0", "memory", 
          "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15"
    );
}

// Generated function: quantize_row_q8_0_baseline
// Config: TuningConfig(use_intrinsics=False, unroll_factor=1, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void quantize_row_q8_0_baseline(const float * GGML_RESTRICT x, block_q8_0 * GGML_RESTRICT y, int64_t k) {
    assert(k % QK8_0 == 0);
    const int64_t nb = k / QK8_0;
    int64_t i = 0;


    // Tail loop
    for (; i < nb; i++) {
        quantize_row_q8_0_rvv_asm_one_block_quantize_row_q8_0_baseline(x + i * QK8_0, &y[i]);
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

#if defined(__RVV_ASM_STD) || defined(__riscv_v)

// Helper function for tail loop - RVV 1.0 版本
static inline void quantize_row_q8_0_rvv_asm_one_block_quantize_row_q8_0_asm_unroll2(const float * GGML_RESTRICT x_ptr, block_q8_0 * GGML_RESTRICT y) {
    float amax;
    // --- Part 1: Calculate amax using RVV ---
    asm volatile (
        "li t0, 32\n\t"
        "vsetvli x0, t0, e32, m1, ta, ma\n\t" 
        "vmv.v.i v1, 0\n\t"

        "vsetvli x0, t0, e32, m8, ta, ma\n\t" 
        "vle32.v v8, (%[x_ptr])\n\t"
        "vfabs.v v8, v8\n\t" 

        "vfredmax.vs v8, v8, v1\n\t"

        "vfmv.f.s %[amax], v8\n\t"
        : [amax] "=f" (amax) : [x_ptr] "r" (x_ptr)
        : "t0", "memory", "v1", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15"
    );
    // --- C++ part: Calculate scaling factor ---
    float d = amax / 127.0f;
    float id = d ? 1.0f / d : 0.0f;
    y->d = GGML_CPU_FP32_TO_FP16(d);
    // --- Part 2: Scale, convert, and store using RVV ---
    asm volatile(
        "li t0, 32\n\t"
        "vsetvli x0, t0, e32, m8, ta, ma\n\t" 
        "vle32.v v8, (%[x_ptr])\n\t"
        "vfmul.vf v8, v8, %[id]\n\t"

        "vsetvli x0, t0, e16, m4, ta, ma\n\t" 
        "vfncvt.x.f.w v4, v8\n\t"

        "vsetvli x0, t0, e8, m2, ta, ma\n\t" 
        "vncvt.x.x.w v2, v4\n\t"

        "vse8.v v2, (%[y_ptr])\n\t"
        : : [x_ptr] "r" (x_ptr), [y_ptr] "r" (y->qs), [id] "f" (id)
        : "t0", "memory", 
          "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15"
    );
}

// Generated function: quantize_row_q8_0_asm_unroll2
// Config: TuningConfig(use_intrinsics=False, unroll_factor=2, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
// Target: RVV 1.0
void quantize_row_q8_0_asm_unroll2(const float * GGML_RESTRICT x, block_q8_0 * GGML_RESTRICT y, int64_t k) {
    assert(k % QK8_0 == 0);
    const int64_t nb = k / QK8_0;
    int64_t i = 0;

    for (; i + 1 < nb; i += 2) {
        float amax[2];
        
        // --- Stage 1: Find amax ---
        asm volatile (
            "li t0, 32\n\t"
            "vsetvli x0, t0, e32, m1, ta, ma\n\t" "vmv.v.i v1, 0\n\t"
            "vsetvli x0, t0, e32, m8, ta, ma\n\t"
            "vle32.v v8, (%[x_ptr0])\n\t"
            "vfabs.v v8, v8\n\t"
            "vfredmax.vs v8, v8, v1\n\t"
            "vfmv.f.s %[amax0], v8\n\t"
            "vle32.v v16, (%[x_ptr1])\n\t"
            "vfabs.v v16, v16\n\t"
            "vfredmax.vs v16, v16, v1\n\t"
            "vfmv.f.s %[amax1], v16\n\t"
            :  [amax0] "=f" (amax[0]),  [amax1] "=f" (amax[1])             :  [x_ptr0] "r" (x + (i + 0) * QK8_0),  [x_ptr1] "r" (x + (i + 1) * QK8_0)             : "t0", "memory", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23"        );

        // --- C++ Interlude ---
        const float d0 = amax[0] / 127.0f;
        const float id0 = d0 ? 1.0f/d0 : 0.0f;
        y[i + 0].d = GGML_FP32_TO_FP16(d0);
        const float d1 = amax[1] / 127.0f;
        const float id1 = d1 ? 1.0f/d1 : 0.0f;
        y[i + 1].d = GGML_FP32_TO_FP16(d1);

        // --- Stage 2: Scale, convert, store ---
        asm volatile(
            "li t0, 32\n\t"
            // Block 0
            "vsetvli x0, t0, e32, m8, ta, ma\n\t"
            "vle32.v v8, (%[x_ptr0])\n\t"
            "vfmul.vf v8, v8, %[id0]\n\t"
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vfncvt.x.f.w v4, v8\n\t"
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vncvt.x.x.w v2, v4\n\t"
            "vse8.v v2, (%[y_ptr0])\n\t"
            // Block 1
            "vsetvli x0, t0, e32, m8, ta, ma\n\t"
            "vle32.v v8, (%[x_ptr1])\n\t"
            "vfmul.vf v8, v8, %[id1]\n\t"
            "vsetvli x0, t0, e16, m4, ta, ma\n\t"
            "vfncvt.x.f.w v4, v8\n\t"
            "vsetvli x0, t0, e8, m2, ta, ma\n\t"
            "vncvt.x.x.w v2, v4\n\t"
            "vse8.v v2, (%[y_ptr1])\n\t"
            :
            :               [x_ptr0] "r" (x + (i + 0) * QK8_0),
              [y_ptr0] "r" (y[i+0].qs),
              [id0] "f" (id0),              [x_ptr1] "r" (x + (i + 1) * QK8_0),
              [y_ptr1] "r" (y[i+1].qs),
              [id1] "f" (id1)            : "t0", "memory", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23"        );
    }

    // Tail loop
    for (; i < nb; i++) {
        quantize_row_q8_0_rvv_asm_one_block_quantize_row_q8_0_asm_unroll2(x + i * QK8_0, &y[i]);
    }
}
#endif


/*
 * -----------------------------------------------------------------------------
 * Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
 * -----------------------------------------------------------------------------
 */

// SKIPPED: Configuration requires vector register v39, but hardware only has 32 (v0-v31).
// Config: TuningConfig(use_intrinsics=False, unroll_factor=4, pipeline_style='serial', use_widening=False, prefetch_distance=0, accumulator_strategy='f32', lmul_strategy='full', use_fused_macc=False, block_batch_size=1)
